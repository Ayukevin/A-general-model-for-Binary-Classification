{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    \n",
    "    x_train = pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0)\n",
    "    y_train = pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0)\n",
    "    x_test = pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0)  \n",
    "    \n",
    "    # Initialize Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    OneHotEncoder_encoder = OneHotEncoder()\n",
    "    standard_encoder = StandardScaler()\n",
    "    minmax_encoder = MinMaxScaler()\n",
    "\n",
    "    # seperate the categorical columns and numerical columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for i in x_train.columns:\n",
    "        if x_train[i].dtype == 'float64':\n",
    "            numerical_columns.append(i)\n",
    "        else:\n",
    "            categorical_columns.append(i)\n",
    "            \n",
    "    # copy to avoid changing the original data\n",
    "    X_train_encoded = x_train.copy()\n",
    "    X_test_encoded = x_test.copy()\n",
    "\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        # Fit the one hot encoder on the combined data of train and test to avoid unseen labels\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        OneHotEncoder_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        # Transform the train and test data\n",
    "        train_encoded = OneHotEncoder_encoder.transform(x_train[col].values.reshape(-1, 1)).toarray()\n",
    "        test_encoded = OneHotEncoder_encoder.transform(x_test[col].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "        # Create new column names for the one hot encoded columns\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=[f\"{col}_{int(i)}\" for i in range(train_encoded.shape[1])])\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=[f\"{col}_{int(i)}\" for i in range(test_encoded.shape[1])])\n",
    "\n",
    "        # Concatenate the new one hot encoded columns to the original dataframe\n",
    "        X_train_encoded = pd.concat([X_train_encoded, train_encoded_df], axis=1)\n",
    "        X_test_encoded = pd.concat([X_test_encoded, test_encoded_df], axis=1)\n",
    "\n",
    "        # Drop the original categorical columns\n",
    "        X_train_encoded.drop(columns=[col], inplace=True)\n",
    "        X_test_encoded.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # for col in categorical_columns:\n",
    "    #     # Fit the label encoder on the combined data of train and test to avoid unseen labels\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     label_encoder.fit(combined_data)\n",
    "\n",
    "    #     X_train_encoded[col] = label_encoder.transform(x_train[col])\n",
    "    #     X_test_encoded[col] = label_encoder.transform(x_test[col])\n",
    "\n",
    "    # #  == apply standard scaler to numerical columns == ##\n",
    "    # for col in numerical_columns:\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     standard_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "    #     X_train_encoded[col] = standard_encoder.fit_transform(x_train[col].values.reshape(-1, 1))\n",
    "    #     X_test_encoded[col] = standard_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    ##  == apply minmax scaler to numerical columns cuz we use XGBOOST and RF== ##\n",
    "    for col in numerical_columns:\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        minmax_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        X_train_encoded[col] = minmax_encoder.transform(x_train[col].values.reshape(-1, 1))\n",
    "        X_test_encoded[col] = minmax_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(X_train_encoded)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import  VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA, X_train shape: (444, 79)\n",
      "Before PCA, y_train shape: (444, 1)\n",
      "After train_test_split, X_train_part shape: (355, 79)\n",
      "After train_test_split, y_train_part shape: (355, 1)\n",
      "After PCA, X_train_pca shape: (355, 34)\n",
      "After PCA, X_valid_pca shape: (89, 34)\n",
      "互信息篩選後的特徵數量: 13\n",
      "[LightGBM] [Info] Number of positive: 109, number of negative: 246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1547\n",
      "[LightGBM] [Info] Number of data points in the train set: 355, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.307042 -> initscore=-0.813984\n",
      "[LightGBM] [Info] Start training from score -0.813984\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM 篩選後的特徵數量: 7\n",
      "Final features in X_train: (444, 7)\n",
      "Final features in X_test: (296, 79)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before PCA, X_train shape: {X_trains[0].shape}\")\n",
    "print(f\"Before PCA, y_train shape: {y_trains[0].shape}\")\n",
    "\n",
    "# Step 1: 切分資料\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "# 確保切分後的樣本數一致\n",
    "print(f\"After train_test_split, X_train_part shape: {X_train_part.shape}\")\n",
    "print(f\"After train_test_split, y_train_part shape: {y_train_part.shape}\")\n",
    "\n",
    "# Step 2: PCA 降維\n",
    "pca = PCA(n_components=0.95)  # 保留 95% 的數據變異\n",
    "X_train_pca = pca.fit_transform(X_train_part)\n",
    "X_valid_pca = pca.transform(X_valid)  # 注意這裡是 transform，不是 fit_transform\n",
    "\n",
    "# 確保降維後樣本數保持一致\n",
    "print(f\"After PCA, X_train_pca shape: {X_train_pca.shape}\")\n",
    "print(f\"After PCA, X_valid_pca shape: {X_valid_pca.shape}\")\n",
    "\n",
    "# Step 3: Mutual Information 特徵選擇\n",
    "mi_scores = mutual_info_classif(X_train_pca, y_train_part.values.ravel())  # 計算互信息分數\n",
    "mi_threshold = np.mean(mi_scores)  # 使用平均值作為閾值\n",
    "selected_mi_features = mi_scores > mi_threshold\n",
    "\n",
    "# 保留選中的特徵\n",
    "X_train_mi = X_train_pca[:, selected_mi_features]\n",
    "X_valid_mi = X_valid_pca[:, selected_mi_features]  # 記得對驗證集也做相同的選擇\n",
    "\n",
    "# 確保選擇後特徵數量正確\n",
    "print(f\"互信息篩選後的特徵數量: {X_train_mi.shape[1]}\")\n",
    "\n",
    "# Step 4: LightGBM 嵌入式方法\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_mi, y_train_part.values.ravel())  # 訓練模型\n",
    "\n",
    "# 使用特徵重要性進行篩選\n",
    "feature_importances = lgb_model.feature_importances_\n",
    "importance_threshold = np.mean(feature_importances)  # 使用平均值作為閾值\n",
    "selected_lgb_features = feature_importances > importance_threshold\n",
    "\n",
    "X_train_final = X_train_mi[:, selected_lgb_features]\n",
    "X_valid_final = X_valid_mi[:, selected_lgb_features]  # 記得對驗證集也進行相同操作\n",
    "\n",
    "# 確保最終特徵數量\n",
    "print(f\"LightGBM 篩選後的特徵數量: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Step 5: 將最終特徵轉回 DataFrame\n",
    "X_train_final_df = pd.DataFrame(X_train_final, columns=[f\"Feature_{i+1}\" for i in range(X_train_final.shape[1])])\n",
    "X_valid_final_df = pd.DataFrame(X_valid_final, columns=[f\"Feature_{i+1}\" for i in range(X_valid_final.shape[1])])\n",
    "\n",
    "# 更新 X_trains 和 X_tests\n",
    "X_trains[0] = pd.concat([X_train_final_df,X_valid_final_df],axis=0) \n",
    "\n",
    "# 顯示最終的特徵\n",
    "print(f\"Final features in X_train: {X_trains[0].shape}\")\n",
    "print(f\"Final features in X_test: {X_tests[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.653443</td>\n",
       "      <td>-0.357158</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>-0.706244</td>\n",
       "      <td>0.432930</td>\n",
       "      <td>0.696367</td>\n",
       "      <td>-0.062739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271724</td>\n",
       "      <td>-0.113929</td>\n",
       "      <td>0.409141</td>\n",
       "      <td>-0.289684</td>\n",
       "      <td>1.026926</td>\n",
       "      <td>-0.457191</td>\n",
       "      <td>0.117750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153021</td>\n",
       "      <td>-0.335995</td>\n",
       "      <td>0.037153</td>\n",
       "      <td>-0.079119</td>\n",
       "      <td>-0.381586</td>\n",
       "      <td>0.316484</td>\n",
       "      <td>0.257436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.080583</td>\n",
       "      <td>-0.233694</td>\n",
       "      <td>0.811494</td>\n",
       "      <td>0.883384</td>\n",
       "      <td>-0.308119</td>\n",
       "      <td>-0.168989</td>\n",
       "      <td>0.181016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.526614</td>\n",
       "      <td>0.556720</td>\n",
       "      <td>-0.301286</td>\n",
       "      <td>0.101057</td>\n",
       "      <td>-0.010348</td>\n",
       "      <td>-0.068873</td>\n",
       "      <td>0.103299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.126303</td>\n",
       "      <td>-0.130561</td>\n",
       "      <td>0.826632</td>\n",
       "      <td>-0.542576</td>\n",
       "      <td>0.057251</td>\n",
       "      <td>0.274762</td>\n",
       "      <td>-0.201547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.173406</td>\n",
       "      <td>0.156356</td>\n",
       "      <td>-0.680375</td>\n",
       "      <td>0.606154</td>\n",
       "      <td>-0.388724</td>\n",
       "      <td>-0.122849</td>\n",
       "      <td>-0.413392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.259985</td>\n",
       "      <td>0.297761</td>\n",
       "      <td>0.028347</td>\n",
       "      <td>-0.180373</td>\n",
       "      <td>-0.207498</td>\n",
       "      <td>-0.202274</td>\n",
       "      <td>-0.534793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.339124</td>\n",
       "      <td>-0.102034</td>\n",
       "      <td>0.291292</td>\n",
       "      <td>-0.173695</td>\n",
       "      <td>-0.255958</td>\n",
       "      <td>0.364220</td>\n",
       "      <td>-0.343519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.603755</td>\n",
       "      <td>0.613188</td>\n",
       "      <td>0.046542</td>\n",
       "      <td>0.232330</td>\n",
       "      <td>-0.046184</td>\n",
       "      <td>-0.158828</td>\n",
       "      <td>0.358996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0   -0.653443  -0.357158   0.294746  -0.706244   0.432930   0.696367   \n",
       "1    1.271724  -0.113929   0.409141  -0.289684   1.026926  -0.457191   \n",
       "2    0.153021  -0.335995   0.037153  -0.079119  -0.381586   0.316484   \n",
       "3   -1.080583  -0.233694   0.811494   0.883384  -0.308119  -0.168989   \n",
       "4    0.526614   0.556720  -0.301286   0.101057  -0.010348  -0.068873   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "84   0.126303  -0.130561   0.826632  -0.542576   0.057251   0.274762   \n",
       "85  -0.173406   0.156356  -0.680375   0.606154  -0.388724  -0.122849   \n",
       "86  -0.259985   0.297761   0.028347  -0.180373  -0.207498  -0.202274   \n",
       "87   0.339124  -0.102034   0.291292  -0.173695  -0.255958   0.364220   \n",
       "88   0.603755   0.613188   0.046542   0.232330  -0.046184  -0.158828   \n",
       "\n",
       "    Feature_7  \n",
       "0   -0.062739  \n",
       "1    0.117750  \n",
       "2    0.257436  \n",
       "3    0.181016  \n",
       "4    0.103299  \n",
       "..        ...  \n",
       "84  -0.201547  \n",
       "85  -0.413392  \n",
       "86  -0.534793  \n",
       "87  -0.343519  \n",
       "88   0.358996  \n",
       "\n",
       "[444 rows x 7 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8_6</th>\n",
       "      <th>Feature_9_9</th>\n",
       "      <th>Feature_10_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_10_3</th>\n",
       "      <th>Feature_10_4</th>\n",
       "      <th>Feature_11_0</th>\n",
       "      <th>Feature_11_1</th>\n",
       "      <th>Feature_11_2</th>\n",
       "      <th>Feature_11_3</th>\n",
       "      <th>Feature_12_6</th>\n",
       "      <th>Feature_13_2</th>\n",
       "      <th>Feature_13_5</th>\n",
       "      <th>Feature_13_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881508</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.388333</td>\n",
       "      <td>0.387764</td>\n",
       "      <td>0.397336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.710368</td>\n",
       "      <td>0.381015</td>\n",
       "      <td>0.495911</td>\n",
       "      <td>0.413570</td>\n",
       "      <td>0.501816</td>\n",
       "      <td>0.571851</td>\n",
       "      <td>0.551714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.685944</td>\n",
       "      <td>0.557729</td>\n",
       "      <td>0.446617</td>\n",
       "      <td>0.457677</td>\n",
       "      <td>0.617032</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.315364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424563</td>\n",
       "      <td>0.478254</td>\n",
       "      <td>0.429935</td>\n",
       "      <td>0.473620</td>\n",
       "      <td>0.509305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.483984</td>\n",
       "      <td>0.339677</td>\n",
       "      <td>0.446617</td>\n",
       "      <td>0.457677</td>\n",
       "      <td>0.420857</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.394755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.710368</td>\n",
       "      <td>0.381015</td>\n",
       "      <td>0.495911</td>\n",
       "      <td>0.413570</td>\n",
       "      <td>0.462188</td>\n",
       "      <td>0.571851</td>\n",
       "      <td>0.551714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.829644</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.562398</td>\n",
       "      <td>0.568736</td>\n",
       "      <td>0.602664</td>\n",
       "      <td>0.340728</td>\n",
       "      <td>0.350418</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0.881508</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.618401</td>\n",
       "      <td>0.518074</td>\n",
       "      <td>0.497854</td>\n",
       "      <td>0.527144</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0.771450</td>\n",
       "      <td>0.221081</td>\n",
       "      <td>0.388333</td>\n",
       "      <td>0.324089</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.408030</td>\n",
       "      <td>0.427106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061147</td>\n",
       "      <td>0.527408</td>\n",
       "      <td>0.436407</td>\n",
       "      <td>0.617032</td>\n",
       "      <td>0.595086</td>\n",
       "      <td>0.474017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0     0.881508   0.154250   0.388333   0.387764   0.397336   0.000000   \n",
       "1     0.710368   0.381015   0.495911   0.413570   0.501816   0.571851   \n",
       "2     0.685944   0.557729   0.446617   0.457677   0.617032   0.350600   \n",
       "3     0.771450   0.000000   0.424563   0.478254   0.429935   0.473620   \n",
       "4     0.483984   0.339677   0.446617   0.457677   0.420857   0.350600   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "439   0.710368   0.381015   0.495911   0.413570   0.462188   0.571851   \n",
       "440   0.829644   0.154250   0.562398   0.568736   0.602664   0.340728   \n",
       "441   0.881508   0.154250   0.618401   0.518074   0.497854   0.527144   \n",
       "442   0.771450   0.221081   0.388333   0.324089   0.577778   0.408030   \n",
       "443   1.000000   0.061147   0.527408   0.436407   0.617032   0.595086   \n",
       "\n",
       "     Feature_7  Feature_8_6  Feature_9_9  Feature_10_0  ...  Feature_10_3  \\\n",
       "0     0.000000          0.0          0.0           0.0  ...           0.0   \n",
       "1     0.551714          1.0          0.0           0.0  ...           0.0   \n",
       "2     0.315364          1.0          0.0           0.0  ...           0.0   \n",
       "3     0.509305          0.0          0.0           1.0  ...           0.0   \n",
       "4     0.394755          0.0          1.0           0.0  ...           0.0   \n",
       "..         ...          ...          ...           ...  ...           ...   \n",
       "439   0.551714          1.0          0.0           0.0  ...           0.0   \n",
       "440   0.350418          1.0          0.0           0.0  ...           1.0   \n",
       "441   0.597408          0.0          0.0           0.0  ...           0.0   \n",
       "442   0.427106          0.0          0.0           0.0  ...           0.0   \n",
       "443   0.474017          1.0          0.0           0.0  ...           0.0   \n",
       "\n",
       "     Feature_10_4  Feature_11_0  Feature_11_1  Feature_11_2  Feature_11_3  \\\n",
       "0             1.0           0.0           0.0           1.0           0.0   \n",
       "1             1.0           0.0           0.0           0.0           1.0   \n",
       "2             0.0           0.0           0.0           1.0           0.0   \n",
       "3             0.0           1.0           0.0           0.0           0.0   \n",
       "4             0.0           1.0           0.0           0.0           0.0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "439           1.0           0.0           1.0           0.0           0.0   \n",
       "440           0.0           0.0           1.0           0.0           0.0   \n",
       "441           1.0           0.0           1.0           0.0           0.0   \n",
       "442           0.0           0.0           0.0           0.0           1.0   \n",
       "443           0.0           0.0           0.0           1.0           0.0   \n",
       "\n",
       "     Feature_12_6  Feature_13_2  Feature_13_5  Feature_13_6  \n",
       "0             0.0           0.0           0.0           0.0  \n",
       "1             1.0           0.0           0.0           0.0  \n",
       "2             1.0           0.0           0.0           0.0  \n",
       "3             0.0           0.0           0.0           0.0  \n",
       "4             0.0           0.0           1.0           0.0  \n",
       "..            ...           ...           ...           ...  \n",
       "439           1.0           0.0           0.0           0.0  \n",
       "440           1.0           0.0           0.0           0.0  \n",
       "441           0.0           0.0           1.0           0.0  \n",
       "442           0.0           1.0           0.0           0.0  \n",
       "443           1.0           0.0           0.0           0.0  \n",
       "\n",
       "[444 rows x 22 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First: use Filter method to remove features with low variance\n",
    "filter = VarianceThreshold()\n",
    "X_train_encoded = X_trains[0]\n",
    "X_test_encoded = X_tests[0]\n",
    "y_train = y_trains[0]   \n",
    "\n",
    "# Fit the filter on the train data\n",
    "filter.fit(X_train_encoded)\n",
    "\n",
    "# Transform the train and test data\n",
    "X_train_encoded = filter.transform(X_train_encoded)\n",
    "X_test_encoded = filter.transform(X_test_encoded)\n",
    "\n",
    "## Second: use embedded method to select features\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_encoded, y_train.values.ravel())\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Select features based on importance\n",
    "selected_features = importances > np.mean(importances)\n",
    "\n",
    "# print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Convert the numpy arrays back to DataFrames with the selected feature names\n",
    "selected_feature_names = X_trains[0].columns[selected_features]\n",
    "X_trains[0] = pd.DataFrame(X_trains[0], columns=selected_feature_names)\n",
    "X_tests[0] = pd.DataFrame(X_tests[0], columns=selected_feature_names)\n",
    "\n",
    "X_trains[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.653443</td>\n",
       "      <td>-0.357158</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>-0.706244</td>\n",
       "      <td>0.432930</td>\n",
       "      <td>0.696367</td>\n",
       "      <td>-0.062739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271724</td>\n",
       "      <td>-0.113929</td>\n",
       "      <td>0.409141</td>\n",
       "      <td>-0.289684</td>\n",
       "      <td>1.026926</td>\n",
       "      <td>-0.457191</td>\n",
       "      <td>0.117750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153021</td>\n",
       "      <td>-0.335995</td>\n",
       "      <td>0.037153</td>\n",
       "      <td>-0.079119</td>\n",
       "      <td>-0.381586</td>\n",
       "      <td>0.316484</td>\n",
       "      <td>0.257436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.080583</td>\n",
       "      <td>-0.233694</td>\n",
       "      <td>0.811494</td>\n",
       "      <td>0.883384</td>\n",
       "      <td>-0.308119</td>\n",
       "      <td>-0.168989</td>\n",
       "      <td>0.181016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.526614</td>\n",
       "      <td>0.556720</td>\n",
       "      <td>-0.301286</td>\n",
       "      <td>0.101057</td>\n",
       "      <td>-0.010348</td>\n",
       "      <td>-0.068873</td>\n",
       "      <td>0.103299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.118497</td>\n",
       "      <td>0.945931</td>\n",
       "      <td>0.564713</td>\n",
       "      <td>0.325568</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>-0.017969</td>\n",
       "      <td>0.038331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.215213</td>\n",
       "      <td>-0.243778</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.249282</td>\n",
       "      <td>-0.162970</td>\n",
       "      <td>0.072015</td>\n",
       "      <td>0.027305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-1.202057</td>\n",
       "      <td>0.177972</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>-0.092066</td>\n",
       "      <td>0.121610</td>\n",
       "      <td>0.165554</td>\n",
       "      <td>0.605229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.945958</td>\n",
       "      <td>0.057737</td>\n",
       "      <td>-0.686733</td>\n",
       "      <td>0.058989</td>\n",
       "      <td>-0.246216</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>0.530199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.276061</td>\n",
       "      <td>1.255240</td>\n",
       "      <td>0.072574</td>\n",
       "      <td>-0.293641</td>\n",
       "      <td>-0.066206</td>\n",
       "      <td>-0.104362</td>\n",
       "      <td>0.023867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    -0.653443  -0.357158   0.294746  -0.706244   0.432930   0.696367   \n",
       "1     1.271724  -0.113929   0.409141  -0.289684   1.026926  -0.457191   \n",
       "2     0.153021  -0.335995   0.037153  -0.079119  -0.381586   0.316484   \n",
       "3    -1.080583  -0.233694   0.811494   0.883384  -0.308119  -0.168989   \n",
       "4     0.526614   0.556720  -0.301286   0.101057  -0.010348  -0.068873   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "350  -0.118497   0.945931   0.564713   0.325568   0.044461  -0.017969   \n",
       "351   0.215213  -0.243778  -0.015757  -0.249282  -0.162970   0.072015   \n",
       "352  -1.202057   0.177972  -0.024658  -0.092066   0.121610   0.165554   \n",
       "353   0.945958   0.057737  -0.686733   0.058989  -0.246216  -0.060297   \n",
       "354  -0.276061   1.255240   0.072574  -0.293641  -0.066206  -0.104362   \n",
       "\n",
       "     Feature_7  \n",
       "0    -0.062739  \n",
       "1     0.117750  \n",
       "2     0.257436  \n",
       "3     0.181016  \n",
       "4     0.103299  \n",
       "..         ...  \n",
       "350   0.038331  \n",
       "351   0.027305  \n",
       "352   0.605229  \n",
       "353   0.530199  \n",
       "354   0.023867  \n",
       "\n",
       "[355 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444, 7)\n",
      "(444, 1)\n",
      "(296, 79)\n"
     ]
    }
   ],
   "source": [
    "print(X_trains[0].shape)\n",
    "print(y_trains[0].shape)\n",
    "print(X_tests[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 times finish\n",
      "AUC: 0.55\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Feature_10_0\n- Feature_10_1\n- Feature_10_2\n- Feature_10_3\n- Feature_10_4\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 使用訓練好的Voting模型來預測X_test並儲存結果\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mstacking_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tests\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     48\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_test_pred, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_predict_proba\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 繪製AUC曲線\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate ROC and AUC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:725\u001b[0m, in \u001b[0;36mStackingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class probabilities for `X` using the final estimator.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \n\u001b[0;32m    712\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m    The class probabilities of the input samples.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    724\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 725\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_estimator_\u001b[38;5;241m.\u001b[39mpredict_proba(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# Handle the multilabel-indicator cases\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([preds[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m preds \u001b[38;5;129;01min\u001b[39;00m y_pred])\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:766\u001b[0m, in \u001b[0;36mStackingClassifier.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return class labels or probabilities for X for each estimator.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m        Prediction outputs for each estimator.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:306\u001b[0m, in \u001b[0;36m_BaseStacking._transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 306\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_method_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_predictions(X, predictions)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    306\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m ]\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_predictions(X, predictions)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:940\u001b[0m, in \u001b[0;36mBaggingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    938\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 940\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[0;32m    949\u001b[0m n_jobs, _, starts \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Feature_10_0\n- Feature_10_1\n- Feature_10_2\n- Feature_10_3\n- Feature_10_4\n- ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "##  Boosting first, Bagging second, Stacking third\n",
    "\n",
    "# 1.Boosting first: Initialize the boosting models\n",
    "boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "boosting_clf3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "# 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "votin_clf= VotingClassifier(estimators=[\n",
    "            ('gb1', boosting_clf1),\n",
    "            ('gb2', boosting_clf2),\n",
    "            ('gb3', boosting_clf3)],voting='soft', n_jobs=-1)\n",
    "\n",
    "\n",
    "bagging_model = BaggingClassifier(estimator=votin_clf,n_estimators=10,random_state=42,n_jobs=-1)\n",
    "\n",
    "# 3.Stacking third: Initialize the stacking model\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('bagging', bagging_model)  # Bagging 模型作為 Stacking 的基分類器之一\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),  # Meta-model\n",
    "    passthrough=True  # 保留原始特徵與基分類器的輸出\n",
    ")\n",
    "\n",
    "# train the stacking model\n",
    "stacking_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "\n",
    "# 使用驗證集進行預測\n",
    "y_pred = stacking_model.predict(X_valid)\n",
    "y_pred_proba = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# 評估模型\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "print(f\"{0} times finish\")\n",
    "print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "# 使用訓練好的Voting模型來預測X_test並儲存結果\n",
    "y_test_pred = stacking_model.predict_proba(X_tests[0])[:, 1]\n",
    "df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "\n",
    "# 繪製AUC曲線\n",
    "# Calculate ROC and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[0]} (AUC = {roc_auc:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_predicts_voting = []\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[i], y_trains[i], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 準備基模型\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))\n",
    "    ]\n",
    "\n",
    "    # 初始化Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "    # 訓練Voting Classifier\n",
    "    voting_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "    # 使用驗證集進行預測\n",
    "    y_pred = voting_model.predict(X_valid)\n",
    "    y_pred_proba = voting_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # 評估模型\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "    print(f\"{i} times finish\")\n",
    "    print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "    # 使用訓練好的Voting模型來預測X_test並儲存結果\n",
    "    y_test_pred = voting_model.predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "    y_predicts_voting.append(df)\n",
    "\n",
    "    # 繪製AUC曲線\n",
    "    # Calculate ROC and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
