{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    \n",
    "    x_train = pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0)\n",
    "    y_train = pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0)\n",
    "    x_test = pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0)  \n",
    "    \n",
    "    # Initialize Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    OneHotEncoder_encoder = OneHotEncoder()\n",
    "    standard_encoder = StandardScaler()\n",
    "    minmax_encoder = MinMaxScaler()\n",
    "\n",
    "    # seperate the categorical columns and numerical columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for i in x_train.columns:\n",
    "        if x_train[i].dtype == 'float64':\n",
    "            numerical_columns.append(i)\n",
    "        else:\n",
    "            categorical_columns.append(i)\n",
    "            \n",
    "    # copy to avoid changing the original data\n",
    "    X_train_encoded = x_train.copy()\n",
    "    X_test_encoded = x_test.copy()\n",
    "\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        # Fit the one hot encoder on the combined data of train and test to avoid unseen labels\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        OneHotEncoder_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        # Transform the train and test data\n",
    "        train_encoded = OneHotEncoder_encoder.transform(x_train[col].values.reshape(-1, 1)).toarray()\n",
    "        test_encoded = OneHotEncoder_encoder.transform(x_test[col].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "        # Create new column names for the one hot encoded columns\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=[f\"{col}_{int(i)}\" for i in range(train_encoded.shape[1])])\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=[f\"{col}_{int(i)}\" for i in range(test_encoded.shape[1])])\n",
    "\n",
    "        # Concatenate the new one hot encoded columns to the original dataframe\n",
    "        X_train_encoded = pd.concat([X_train_encoded, train_encoded_df], axis=1)\n",
    "        X_test_encoded = pd.concat([X_test_encoded, test_encoded_df], axis=1)\n",
    "\n",
    "        # Drop the original categorical columns\n",
    "        X_train_encoded.drop(columns=[col], inplace=True)\n",
    "        X_test_encoded.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # for col in categorical_columns:\n",
    "    #     # Fit the label encoder on the combined data of train and test to avoid unseen labels\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     label_encoder.fit(combined_data)\n",
    "\n",
    "    #     X_train_encoded[col] = label_encoder.transform(x_train[col])\n",
    "    #     X_test_encoded[col] = label_encoder.transform(x_test[col])\n",
    "\n",
    "    # #  == apply standard scaler to numerical columns == ##\n",
    "    # for col in numerical_columns:\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     standard_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "    #     X_train_encoded[col] = standard_encoder.fit_transform(x_train[col].values.reshape(-1, 1))\n",
    "    #     X_test_encoded[col] = standard_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    ##  == apply minmax scaler to numerical columns cuz we use XGBOOST and RF== ##\n",
    "    for col in numerical_columns:\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        minmax_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        X_train_encoded[col] = minmax_encoder.transform(x_train[col].values.reshape(-1, 1))\n",
    "        X_test_encoded[col] = minmax_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(X_train_encoded)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import  VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA, X_train shape: (444, 79)\n",
      "Before PCA, y_train shape: (444, 1)\n",
      "After train_test_split, X_train_part shape: (355, 79)\n",
      "After train_test_split, y_train_part shape: (355, 1)\n",
      "After PCA, X_train_pca shape: (355, 34)\n",
      "After PCA, X_valid_pca shape: (89, 34)\n",
      "互信息篩選後的特徵數量: 13\n",
      "[LightGBM] [Info] Number of positive: 109, number of negative: 246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1547\n",
      "[LightGBM] [Info] Number of data points in the train set: 355, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.307042 -> initscore=-0.813984\n",
      "[LightGBM] [Info] Start training from score -0.813984\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM 篩選後的特徵數量: 7\n",
      "Final features in X_train: (355, 7)\n",
      "Final features in X_test: (89, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before PCA, X_train shape: {X_trains[0].shape}\")\n",
    "print(f\"Before PCA, y_train shape: {y_trains[0].shape}\")\n",
    "\n",
    "# Step 1: 切分資料\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "# 確保切分後的樣本數一致\n",
    "print(f\"After train_test_split, X_train_part shape: {X_train_part.shape}\")\n",
    "print(f\"After train_test_split, y_train_part shape: {y_train_part.shape}\")\n",
    "\n",
    "# Step 2: PCA 降維\n",
    "pca = PCA(n_components=0.95)  # 保留 95% 的數據變異\n",
    "X_train_pca = pca.fit_transform(X_train_part)\n",
    "X_valid_pca = pca.transform(X_valid)  # 注意這裡是 transform，不是 fit_transform\n",
    "\n",
    "# 確保降維後樣本數保持一致\n",
    "print(f\"After PCA, X_train_pca shape: {X_train_pca.shape}\")\n",
    "print(f\"After PCA, X_valid_pca shape: {X_valid_pca.shape}\")\n",
    "\n",
    "# Step 3: Mutual Information 特徵選擇\n",
    "mi_scores = mutual_info_classif(X_train_pca, y_train_part.values.ravel())  # 計算互信息分數\n",
    "mi_threshold = np.mean(mi_scores)  # 使用平均值作為閾值\n",
    "selected_mi_features = mi_scores > mi_threshold\n",
    "\n",
    "# 保留選中的特徵\n",
    "X_train_mi = X_train_pca[:, selected_mi_features]\n",
    "X_valid_mi = X_valid_pca[:, selected_mi_features]  # 記得對驗證集也做相同的選擇\n",
    "\n",
    "# 確保選擇後特徵數量正確\n",
    "print(f\"互信息篩選後的特徵數量: {X_train_mi.shape[1]}\")\n",
    "\n",
    "# Step 4: LightGBM 嵌入式方法\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_mi, y_train_part.values.ravel())  # 訓練模型\n",
    "\n",
    "# 使用特徵重要性進行篩選\n",
    "feature_importances = lgb_model.feature_importances_\n",
    "importance_threshold = np.mean(feature_importances)  # 使用平均值作為閾值\n",
    "selected_lgb_features = feature_importances > importance_threshold\n",
    "\n",
    "X_train_final = X_train_mi[:, selected_lgb_features]\n",
    "X_valid_final = X_valid_mi[:, selected_lgb_features]  # 記得對驗證集也進行相同操作\n",
    "\n",
    "# 確保最終特徵數量\n",
    "print(f\"LightGBM 篩選後的特徵數量: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Step 5: 將最終特徵轉回 DataFrame\n",
    "X_train_final_df = pd.DataFrame(X_train_final, columns=[f\"Feature_{i+1}\" for i in range(X_train_final.shape[1])])\n",
    "X_valid_final_df = pd.DataFrame(X_valid_final, columns=[f\"Feature_{i+1}\" for i in range(X_valid_final.shape[1])])\n",
    "\n",
    "# 更新 X_trains 和 X_tests\n",
    "X_trains[0] = X_train_final_df\n",
    "X_tests[0] = X_valid_final_df\n",
    "\n",
    "# 顯示最終的特徵\n",
    "print(f\"Final features in X_train: {X_trains[0].shape}\")\n",
    "print(f\"Final features in X_test: {X_tests[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA, X_train shape: (444, 79)\n",
      "Before PCA, y_train shape: (444, 1)\n",
      "after pca: 34\n",
      "After PCA, X_train_pca shape: (355, 34)\n",
      "After PCA, y_train shape: (444, 1)\n",
      "互信息篩選後的特徵數量: 13\n",
      "[LightGBM] [Info] Number of positive: 109, number of negative: 246\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1547\n",
      "[LightGBM] [Info] Number of data points in the train set: 355, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.307042 -> initscore=-0.813984\n",
      "[LightGBM] [Info] Start training from score -0.813984\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM 篩選後的特徵數量: 7\n",
      "Final features in X_train: (355, 7)\n",
      "Final features in X_test: (89, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before PCA, X_train shape: {X_trains[0].shape}\")\n",
    "print(f\"Before PCA, y_train shape: {y_trains[0].shape}\")\n",
    "\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: PCA 降維\n",
    "pca = PCA(n_components=0.95)  # 保留 95% 的數據變異\n",
    "X_train_pca = pca.fit_transform(X_train_part)\n",
    "X_valid_pca = pca.transform(X_valid)\n",
    "\n",
    "print(f\"after pca: {X_train_pca.shape[1]}\")\n",
    "\n",
    "print(f\"After PCA, X_train_pca shape: {X_train_pca.shape}\")\n",
    "print(f\"After PCA, y_train shape: {y_trains[0].shape}\")\n",
    "\n",
    "# Step 2: Mutual Information 特徵選擇\n",
    "mi_scores = mutual_info_classif(X_train_pca, y_train_part.values.ravel())  # 計算互信息分數\n",
    "mi_threshold = np.mean(mi_scores)  # 使用平均值作為閾值\n",
    "selected_mi_features = mi_scores > mi_threshold\n",
    "\n",
    "# 保留選中的特徵\n",
    "X_train_mi = X_train_pca[:, selected_mi_features]\n",
    "X_valid_mi = X_valid_pca[:, selected_mi_features]\n",
    "\n",
    "print(f\"互信息篩選後的特徵數量: {X_train_mi.shape[1]}\")\n",
    "\n",
    "# Step 3: 嵌入式方法 LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_mi, y_train_part.values.ravel())  # 訓練模型\n",
    "\n",
    "# 使用特徵重要性進行篩選\n",
    "feature_importances = lgb_model.feature_importances_\n",
    "importance_threshold = np.mean(feature_importances)  # 使用平均值作為閾值\n",
    "selected_lgb_features = feature_importances > importance_threshold\n",
    "\n",
    "X_train_final = X_train_mi[:, selected_lgb_features]\n",
    "X_valid_final = X_valid_mi[:, selected_lgb_features]\n",
    "\n",
    "print(f\"LightGBM 篩選後的特徵數量: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Convert the numpy arrays back to DataFrames with the selected feature names\n",
    "X_train_final_df = pd.DataFrame(X_train_final, columns=[f\"Feature_{i+1}\" for i in range(X_train_final.shape[1])])\n",
    "X_valid_final_df = pd.DataFrame(X_valid_final, columns=[f\"Feature_{i+1}\" for i in range(X_valid_final.shape[1])])\n",
    "\n",
    "# Update X_trains[0] and X_tests[0] with the new DataFrames\n",
    "X_trains[0] = X_train_final_df\n",
    "X_tests[0] = X_valid_final_df\n",
    "\n",
    "# 顯示最終的特徵\n",
    "print(f\"Final features in X_train: {X_trains[0].shape}\")\n",
    "print(f\"Final features in X_test: {X_tests[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8_6</th>\n",
       "      <th>Feature_9_9</th>\n",
       "      <th>Feature_10_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_10_3</th>\n",
       "      <th>Feature_10_4</th>\n",
       "      <th>Feature_11_0</th>\n",
       "      <th>Feature_11_1</th>\n",
       "      <th>Feature_11_2</th>\n",
       "      <th>Feature_11_3</th>\n",
       "      <th>Feature_12_6</th>\n",
       "      <th>Feature_13_2</th>\n",
       "      <th>Feature_13_5</th>\n",
       "      <th>Feature_13_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881508</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.388333</td>\n",
       "      <td>0.387764</td>\n",
       "      <td>0.397336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.710368</td>\n",
       "      <td>0.381015</td>\n",
       "      <td>0.495911</td>\n",
       "      <td>0.413570</td>\n",
       "      <td>0.501816</td>\n",
       "      <td>0.571851</td>\n",
       "      <td>0.551714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.685944</td>\n",
       "      <td>0.557729</td>\n",
       "      <td>0.446617</td>\n",
       "      <td>0.457677</td>\n",
       "      <td>0.617032</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.315364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424563</td>\n",
       "      <td>0.478254</td>\n",
       "      <td>0.429935</td>\n",
       "      <td>0.473620</td>\n",
       "      <td>0.509305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.483984</td>\n",
       "      <td>0.339677</td>\n",
       "      <td>0.446617</td>\n",
       "      <td>0.457677</td>\n",
       "      <td>0.420857</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.394755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.710368</td>\n",
       "      <td>0.381015</td>\n",
       "      <td>0.495911</td>\n",
       "      <td>0.413570</td>\n",
       "      <td>0.462188</td>\n",
       "      <td>0.571851</td>\n",
       "      <td>0.551714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.829644</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.562398</td>\n",
       "      <td>0.568736</td>\n",
       "      <td>0.602664</td>\n",
       "      <td>0.340728</td>\n",
       "      <td>0.350418</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0.881508</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.618401</td>\n",
       "      <td>0.518074</td>\n",
       "      <td>0.497854</td>\n",
       "      <td>0.527144</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0.771450</td>\n",
       "      <td>0.221081</td>\n",
       "      <td>0.388333</td>\n",
       "      <td>0.324089</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.408030</td>\n",
       "      <td>0.427106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061147</td>\n",
       "      <td>0.527408</td>\n",
       "      <td>0.436407</td>\n",
       "      <td>0.617032</td>\n",
       "      <td>0.595086</td>\n",
       "      <td>0.474017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0     0.881508   0.154250   0.388333   0.387764   0.397336   0.000000   \n",
       "1     0.710368   0.381015   0.495911   0.413570   0.501816   0.571851   \n",
       "2     0.685944   0.557729   0.446617   0.457677   0.617032   0.350600   \n",
       "3     0.771450   0.000000   0.424563   0.478254   0.429935   0.473620   \n",
       "4     0.483984   0.339677   0.446617   0.457677   0.420857   0.350600   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "439   0.710368   0.381015   0.495911   0.413570   0.462188   0.571851   \n",
       "440   0.829644   0.154250   0.562398   0.568736   0.602664   0.340728   \n",
       "441   0.881508   0.154250   0.618401   0.518074   0.497854   0.527144   \n",
       "442   0.771450   0.221081   0.388333   0.324089   0.577778   0.408030   \n",
       "443   1.000000   0.061147   0.527408   0.436407   0.617032   0.595086   \n",
       "\n",
       "     Feature_7  Feature_8_6  Feature_9_9  Feature_10_0  ...  Feature_10_3  \\\n",
       "0     0.000000          0.0          0.0           0.0  ...           0.0   \n",
       "1     0.551714          1.0          0.0           0.0  ...           0.0   \n",
       "2     0.315364          1.0          0.0           0.0  ...           0.0   \n",
       "3     0.509305          0.0          0.0           1.0  ...           0.0   \n",
       "4     0.394755          0.0          1.0           0.0  ...           0.0   \n",
       "..         ...          ...          ...           ...  ...           ...   \n",
       "439   0.551714          1.0          0.0           0.0  ...           0.0   \n",
       "440   0.350418          1.0          0.0           0.0  ...           1.0   \n",
       "441   0.597408          0.0          0.0           0.0  ...           0.0   \n",
       "442   0.427106          0.0          0.0           0.0  ...           0.0   \n",
       "443   0.474017          1.0          0.0           0.0  ...           0.0   \n",
       "\n",
       "     Feature_10_4  Feature_11_0  Feature_11_1  Feature_11_2  Feature_11_3  \\\n",
       "0             1.0           0.0           0.0           1.0           0.0   \n",
       "1             1.0           0.0           0.0           0.0           1.0   \n",
       "2             0.0           0.0           0.0           1.0           0.0   \n",
       "3             0.0           1.0           0.0           0.0           0.0   \n",
       "4             0.0           1.0           0.0           0.0           0.0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "439           1.0           0.0           1.0           0.0           0.0   \n",
       "440           0.0           0.0           1.0           0.0           0.0   \n",
       "441           1.0           0.0           1.0           0.0           0.0   \n",
       "442           0.0           0.0           0.0           0.0           1.0   \n",
       "443           0.0           0.0           0.0           1.0           0.0   \n",
       "\n",
       "     Feature_12_6  Feature_13_2  Feature_13_5  Feature_13_6  \n",
       "0             0.0           0.0           0.0           0.0  \n",
       "1             1.0           0.0           0.0           0.0  \n",
       "2             1.0           0.0           0.0           0.0  \n",
       "3             0.0           0.0           0.0           0.0  \n",
       "4             0.0           0.0           1.0           0.0  \n",
       "..            ...           ...           ...           ...  \n",
       "439           1.0           0.0           0.0           0.0  \n",
       "440           1.0           0.0           0.0           0.0  \n",
       "441           0.0           0.0           1.0           0.0  \n",
       "442           0.0           1.0           0.0           0.0  \n",
       "443           1.0           0.0           0.0           0.0  \n",
       "\n",
       "[444 rows x 22 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First: use Filter method to remove features with low variance\n",
    "filter = VarianceThreshold()\n",
    "X_train_encoded = X_trains[0]\n",
    "X_test_encoded = X_tests[0]\n",
    "y_train = y_trains[0]   \n",
    "\n",
    "# Fit the filter on the train data\n",
    "filter.fit(X_train_encoded)\n",
    "\n",
    "# Transform the train and test data\n",
    "X_train_encoded = filter.transform(X_train_encoded)\n",
    "X_test_encoded = filter.transform(X_test_encoded)\n",
    "\n",
    "## Second: use embedded method to select features\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_encoded, y_train.values.ravel())\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Select features based on importance\n",
    "selected_features = importances > np.mean(importances)\n",
    "\n",
    "# print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Convert the numpy arrays back to DataFrames with the selected feature names\n",
    "selected_feature_names = X_trains[0].columns[selected_features]\n",
    "X_trains[0] = pd.DataFrame(X_trains[0], columns=selected_feature_names)\n",
    "X_tests[0] = pd.DataFrame(X_tests[0], columns=selected_feature_names)\n",
    "\n",
    "X_trains[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.653443</td>\n",
       "      <td>-0.357158</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>-0.706244</td>\n",
       "      <td>0.432930</td>\n",
       "      <td>0.696367</td>\n",
       "      <td>-0.062739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.271724</td>\n",
       "      <td>-0.113929</td>\n",
       "      <td>0.409141</td>\n",
       "      <td>-0.289684</td>\n",
       "      <td>1.026926</td>\n",
       "      <td>-0.457191</td>\n",
       "      <td>0.117750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153021</td>\n",
       "      <td>-0.335995</td>\n",
       "      <td>0.037153</td>\n",
       "      <td>-0.079119</td>\n",
       "      <td>-0.381586</td>\n",
       "      <td>0.316484</td>\n",
       "      <td>0.257436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.080583</td>\n",
       "      <td>-0.233694</td>\n",
       "      <td>0.811494</td>\n",
       "      <td>0.883384</td>\n",
       "      <td>-0.308119</td>\n",
       "      <td>-0.168989</td>\n",
       "      <td>0.181016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.526614</td>\n",
       "      <td>0.556720</td>\n",
       "      <td>-0.301286</td>\n",
       "      <td>0.101057</td>\n",
       "      <td>-0.010348</td>\n",
       "      <td>-0.068873</td>\n",
       "      <td>0.103299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.118497</td>\n",
       "      <td>0.945931</td>\n",
       "      <td>0.564713</td>\n",
       "      <td>0.325568</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>-0.017969</td>\n",
       "      <td>0.038331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.215213</td>\n",
       "      <td>-0.243778</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.249282</td>\n",
       "      <td>-0.162970</td>\n",
       "      <td>0.072015</td>\n",
       "      <td>0.027305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-1.202057</td>\n",
       "      <td>0.177972</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>-0.092066</td>\n",
       "      <td>0.121610</td>\n",
       "      <td>0.165554</td>\n",
       "      <td>0.605229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.945958</td>\n",
       "      <td>0.057737</td>\n",
       "      <td>-0.686733</td>\n",
       "      <td>0.058989</td>\n",
       "      <td>-0.246216</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>0.530199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.276061</td>\n",
       "      <td>1.255240</td>\n",
       "      <td>0.072574</td>\n",
       "      <td>-0.293641</td>\n",
       "      <td>-0.066206</td>\n",
       "      <td>-0.104362</td>\n",
       "      <td>0.023867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    -0.653443  -0.357158   0.294746  -0.706244   0.432930   0.696367   \n",
       "1     1.271724  -0.113929   0.409141  -0.289684   1.026926  -0.457191   \n",
       "2     0.153021  -0.335995   0.037153  -0.079119  -0.381586   0.316484   \n",
       "3    -1.080583  -0.233694   0.811494   0.883384  -0.308119  -0.168989   \n",
       "4     0.526614   0.556720  -0.301286   0.101057  -0.010348  -0.068873   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "350  -0.118497   0.945931   0.564713   0.325568   0.044461  -0.017969   \n",
       "351   0.215213  -0.243778  -0.015757  -0.249282  -0.162970   0.072015   \n",
       "352  -1.202057   0.177972  -0.024658  -0.092066   0.121610   0.165554   \n",
       "353   0.945958   0.057737  -0.686733   0.058989  -0.246216  -0.060297   \n",
       "354  -0.276061   1.255240   0.072574  -0.293641  -0.066206  -0.104362   \n",
       "\n",
       "     Feature_7  \n",
       "0    -0.062739  \n",
       "1     0.117750  \n",
       "2     0.257436  \n",
       "3     0.181016  \n",
       "4     0.103299  \n",
       "..         ...  \n",
       "350   0.038331  \n",
       "351   0.027305  \n",
       "352   0.605229  \n",
       "353   0.530199  \n",
       "354   0.023867  \n",
       "\n",
       "[355 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355, 7)\n",
      "(444, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_trains[0].shape)\n",
    "print(y_trains[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [355, 444]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m X_train_part, X_valid, y_train_part, y_valid \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trains\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trains\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m##  Boosting first, Bagging second, Stacking third\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1.Boosting first: Initialize the boosting models\u001b[39;00m\n\u001b[0;32m      8\u001b[0m boosting_clf1 \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2782\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2786\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\user\\OneDrive\\桌面\\大四上\\A-general-model-for-Binary-Classification\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [355, 444]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "##  Boosting first, Bagging second, Stacking third\n",
    "\n",
    "# 1.Boosting first: Initialize the boosting models\n",
    "boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "boosting_clf3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "# 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "votin_clf= VotingClassifier(estimators=[\n",
    "            ('gb1', boosting_clf1),\n",
    "            ('gb2', boosting_clf2),\n",
    "            ('gb3', boosting_clf3)],voting='soft', n_jobs=-1)\n",
    "\n",
    "\n",
    "bagging_model = BaggingClassifier(estimator=votin_clf,n_estimators=10,random_state=42,n_jobs=-1)\n",
    "\n",
    "# 3.Stacking third: Initialize the stacking model\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('bagging', bagging_model)  # Bagging 模型作為 Stacking 的基分類器之一\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),  # Meta-model\n",
    "    passthrough=True  # 保留原始特徵與基分類器的輸出\n",
    ")\n",
    "\n",
    "# train the stacking model\n",
    "stacking_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "\n",
    "# 使用驗證集進行預測\n",
    "y_pred = stacking_model.predict(X_valid)\n",
    "y_pred_proba = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# 評估模型\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "print(f\"{0} times finish\")\n",
    "print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "# 使用訓練好的Voting模型來預測X_test並儲存結果\n",
    "y_test_pred = stacking_model.predict_proba(X_tests[0])[:, 1]\n",
    "df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "\n",
    "# 繪製AUC曲線\n",
    "# Calculate ROC and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[0]} (AUC = {roc_auc:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_predicts_voting = []\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[i], y_trains[i], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 準備基模型\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))\n",
    "    ]\n",
    "\n",
    "    # 初始化Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "    # 訓練Voting Classifier\n",
    "    voting_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "    # 使用驗證集進行預測\n",
    "    y_pred = voting_model.predict(X_valid)\n",
    "    y_pred_proba = voting_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # 評估模型\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "    print(f\"{i} times finish\")\n",
    "    print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "    # 使用訓練好的Voting模型來預測X_test並儲存結果\n",
    "    y_test_pred = voting_model.predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "    y_predicts_voting.append(df)\n",
    "\n",
    "    # 繪製AUC曲線\n",
    "    # Calculate ROC and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
