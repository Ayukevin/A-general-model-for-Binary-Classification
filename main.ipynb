{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to use `LabelEncoder` and `OneHotEncoder` to transform categorical data, and `MinMaxScaler` and `StandardScaler` to transform numerical data. After several trials, we decided to use `OneHotEncoder` for categorical data and `MinMaxScaler` for numerical data. The reasons are as follows:\n",
    "\n",
    "1. Categorical data does not have an inherent order. Using `LabelEncoder` might inadvertently create ordinal features.\n",
    "2. Numerical data exhibits significant variance across columns. Using `StandardScaler` might reduce the 'distance' features, also it's better for  XGBOOST and RF\n",
    "\n",
    "Therefore, we opted for `OneHotEncoder` and `MinMaxScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to store the data\n",
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    \n",
    "    # read the data\n",
    "    x_train = pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0)\n",
    "    y_train = pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0)\n",
    "    x_test = pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0)  \n",
    "    \n",
    "    # Initialize Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    OneHotEncoder_encoder = OneHotEncoder()\n",
    "    standard_encoder = StandardScaler()\n",
    "    minmax_encoder = MinMaxScaler()\n",
    "\n",
    "    # seperate the categorical columns and numerical columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for i in x_train.columns:\n",
    "        if x_train[i].dtype == 'float64':\n",
    "            numerical_columns.append(i) #'float64' is the data type of numerical columns\n",
    "        else:\n",
    "            categorical_columns.append(i) #the other type is the data type of categorical columns\n",
    "            \n",
    "    # copy to avoid changing the original data\n",
    "    X_train_encoded = x_train.copy()\n",
    "    X_test_encoded = x_test.copy()\n",
    "\n",
    "    ## == apply one hot encoding to categorical columns == ##\n",
    "    ## =================================================== ##\n",
    "    for col in categorical_columns:\n",
    "        # Fit the one hot encoder on the combined data of train and test to avoid unseen labels\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        OneHotEncoder_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        # Transform the train and test data\n",
    "        ## reshape(-1, 1) is used to convert the 1D array to 2D array\n",
    "        ## The one hot encoder returns a sparse matrix, so we convert it to a dense matrix\n",
    "        train_encoded = OneHotEncoder_encoder.transform(x_train[col].values.reshape(-1, 1)).toarray()\n",
    "        test_encoded = OneHotEncoder_encoder.transform(x_test[col].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "        # Create new column names for the one hot encoded columns\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=[f\"{col}_{int(i)}\" for i in range(train_encoded.shape[1])])\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=[f\"{col}_{int(i)}\" for i in range(test_encoded.shape[1])])\n",
    "\n",
    "        # Concatenate the new one hot encoded columns to the original dataframe\n",
    "        X_train_encoded = pd.concat([X_train_encoded, train_encoded_df], axis=1)\n",
    "        X_test_encoded = pd.concat([X_test_encoded, test_encoded_df], axis=1)\n",
    "\n",
    "        # Drop the original categorical columns after encoding\n",
    "        X_train_encoded.drop(columns=[col], inplace=True)\n",
    "        X_test_encoded.drop(columns=[col], inplace=True)\n",
    "\n",
    "    ## == apply minmax scaler to numerical columns cuz it's better for  XGBOOST and RF== ##\n",
    "    ## ================================================================================= ##\n",
    "    for col in numerical_columns:\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        minmax_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        X_train_encoded[col] = minmax_encoder.transform(x_train[col].values.reshape(-1, 1))\n",
    "        X_test_encoded[col] = minmax_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    # input the data to the list \n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(X_train_encoded)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test_encoded)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the encoded rightly input in the X_trains or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import  VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VarianceThreshold and  embedded method\n",
    "use VarianceThreshold to remove features with low variance and RF to pick the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_names)):\n",
    "\n",
    "    ## First: use Filter method to remove features with low variance\n",
    "    filter = VarianceThreshold()\n",
    "    X_train_encoded = X_trains[i]\n",
    "    X_test_encoded = X_tests[i]\n",
    "    y_train = y_trains[i]   \n",
    "\n",
    "    # Fit the filter on the train data\n",
    "    filter.fit(X_train_encoded)\n",
    "\n",
    "    # Transform the train and test data\n",
    "    X_train_encoded = filter.transform(X_train_encoded)\n",
    "    X_test_encoded = filter.transform(X_test_encoded)\n",
    "\n",
    "    ## Second: use embedded method to select features\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_encoded, y_train.values.ravel())\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Select features based on importance\n",
    "    selected_features = importances > np.mean(importances)\n",
    "\n",
    "    # Convert the numpy arrays back to DataFrames with the selected feature names\n",
    "    selected_feature_names = X_trains[i].columns[filter.get_support()][selected_features]\n",
    "    X_trains[i] = pd.DataFrame(X_train_encoded[:, selected_features], columns=selected_feature_names)\n",
    "    X_tests[i] = pd.DataFrame(X_test_encoded[:, selected_features], columns=selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclustion: \n",
    "1. ignore the interact effect between features\n",
    "2. choose improperly\n",
    "\n",
    "=> rejec tot use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA +Mutual Information +lightgbm\n",
    "Good way to reduct dimension and let 'Mutual Information' to keep interaction effect\n",
    "At least, use 'lightgbm'to pick the important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 1. use PCA to reduce the dimensionality of the data  #\n",
    "pca = PCA(n_components=0.95)  \n",
    "X_train_pca = pca.fit_transform(X_train_part)\n",
    "X_valid_pca = pca.transform(X_valid) \n",
    "\n",
    "\n",
    "# 2. use Mutual Information to select high-quality features  #\n",
    "## use mean score as threshold\n",
    "mi_scores = mutual_info_classif(X_train_pca, y_train_part.values.ravel())  \n",
    "mi_threshold = np.mean(mi_scores)  \n",
    "selected_mi_features = mi_scores > mi_threshold\n",
    "\n",
    "## keep the selected features\n",
    "X_train_mi = X_train_pca[:, selected_mi_features]\n",
    "X_valid_mi = X_valid_pca[:, selected_mi_features]  \n",
    "\n",
    "\n",
    "# 3. use LightGBM to pick the best features  #\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_mi, y_train_part.values.ravel()) \n",
    "\n",
    "## get the feature importances and select the features with importance higher than the mean importance\n",
    "feature_importances = lgb_model.feature_importances_\n",
    "importance_threshold = np.mean(feature_importances)  \n",
    "selected_lgb_features = feature_importances > importance_threshold\n",
    "\n",
    "## keep the selected features\n",
    "X_train_final = X_train_mi[:, selected_lgb_features]\n",
    "X_valid_final = X_valid_mi[:, selected_lgb_features]  \n",
    "\n",
    "\n",
    "# 4. input the selected features back to DataFrame\n",
    "X_train_final_df = pd.DataFrame(X_train_final, columns=[f\"Feature_{i+1}\" for i in range(X_train_final.shape[1])])\n",
    "X_valid_final_df = pd.DataFrame(X_valid_final, columns=[f\"Feature_{i+1}\" for i in range(X_valid_final.shape[1])])\n",
    "\n",
    "# 更新 X_trains 和 X_tests\n",
    "X_trains[0] = pd.concat([X_train_final_df,X_valid_final_df],axis=0) \n",
    "X_tests[0] = pd.concat([X_train_final_df,X_valid_final_df],axis=0)  #有問題需要修改\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclustion: \n",
    "We don't know why the sample size of x train decrease after the PCA and can't find out where is the problem\n",
    "\n",
    "=> rejec tot use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RFECV\n",
    "recursively eliminate features and, in combination with cross-validation, to select the optimal feature subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_names)):\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Initialize RFECV\n",
    "    rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "\n",
    "    x_train = X_trains[i]\n",
    "    y_train = y_trains[i]\n",
    "\n",
    "    # Fit RFECV\n",
    "    rfecv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features = x_train.columns[rfecv.support_]\n",
    "\n",
    "    # print(\"Selected features:\", selected_features)\n",
    "\n",
    "    # Transform the datasets\n",
    "    X_trains[i] = X_trains[i][selected_features]\n",
    "    X_tests[i] = X_tests[i][selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclustion: \n",
    "wasting too much time\n",
    "\n",
    "=> rejec tot use\n",
    "\n",
    "#### Therefore, we give up to use Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split & build Model\n",
    "select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strategy: Boosting first, Bagging second, Stacking third\n",
    "1. Boosting first: use mulitple classifier as base classifiers\n",
    "2. Bagging second: enhance stability and reduce variance through Bagging with random sampling\n",
    "3. Stacking third: use the output of Bagging as features to train a final meta-model for the ultimate prediction. We choose LogisticRegression to be final meta-model\n",
    "\n",
    "Therefore, we try different combination of Boosting clf to get a better AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Boosting first: Initialize the boosting models\n",
    "boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "boosting_clf3 = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "boosting_clf4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "boosting_clf5 = CatBoostClassifier(iterations=100, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "votin_clf= VotingClassifier(estimators=[\n",
    "            ('gb1', boosting_clf1),\n",
    "            ('gb2', boosting_clf2),\n",
    "            ('gb3', boosting_clf3),\n",
    "            ('gb4', boosting_clf4),\n",
    "            ('gb5', boosting_clf5),],voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Boosting first: Initialize the boosting models\n",
    "boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "boosting_clf3 = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "boosting_clf4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "boosting_clf5 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "boosting_clf6 = SVC(probability=True, random_state=42)\n",
    "\n",
    "\n",
    "# 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "votin_clf= VotingClassifier(estimators=[\n",
    "            ('gb1', boosting_clf1),\n",
    "            ('gb2', boosting_clf2),\n",
    "            ('gb3', boosting_clf3),\n",
    "            ('gb4', boosting_clf4),\n",
    "            ('gb5', boosting_clf5),\n",
    "            ('gb6', boosting_clf6),],voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Boosting first: Initialize the boosting models\n",
    "boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "boosting_clf3 = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "boosting_clf4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "boosting_clf5 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "boosting_clf6 = SVC(probability=True, random_state=42)\n",
    "boosting_clf8 = GaussianNB()\n",
    "boosting_clf10 = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "votin_clf= VotingClassifier(estimators=[\n",
    "        ('gb1', boosting_clf1),\n",
    "        ('gb2', boosting_clf2),\n",
    "        ('gb3', boosting_clf3),\n",
    "        ('gb4', boosting_clf4),\n",
    "        ('gb5', boosting_clf5),\n",
    "        ('gb6', boosting_clf6),\n",
    "        ('gb8', boosting_clf8),\n",
    "        ('gb10', boosting_clf10)],voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and We finally find this way could perform the best AUC score and get auc score 0.863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicts_voting = []\n",
    "plt.figure(figsize=(10, 8)) # plot AUC curve to check the performance of the model\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[i], y_trains[i], test_size=0.2, random_state=42)\n",
    "    \n",
    "    ##  Boosting first, Bagging second, Stacking third\n",
    "\n",
    "    # 1.Boosting first: Initialize the boosting models\n",
    "    boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "    boosting_clf3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    # 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "    votin_clf= VotingClassifier(estimators=[\n",
    "                ('gb1', boosting_clf1),\n",
    "                ('gb2', boosting_clf2),\n",
    "                ('gb3', boosting_clf3),],voting='soft', n_jobs=-1)\n",
    "\n",
    "\n",
    "    bagging_model = BaggingClassifier(estimator=votin_clf,n_estimators=10,random_state=42,n_jobs=-1)\n",
    "\n",
    "    # 3.Stacking third: Initialize the stacking model\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[ ('bagging', bagging_model)],# use the bagging model as the base model\n",
    "        final_estimator=LogisticRegression(),  # Meta-model\n",
    "        passthrough=True  # Retain the original features and the output of the base classifiers.\n",
    "    )\n",
    "\n",
    "    # train the stacking model\n",
    "    stacking_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "\n",
    "    # predict the validation set\n",
    "    y_pred = stacking_model.predict(X_valid)\n",
    "    y_pred_proba = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # Calculate the accuracy and AUC\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "    print(f\"{i} times finish\")\n",
    "    print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "    # store the prediction results\n",
    "    y_test_pred = stacking_model.predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "    y_predicts_voting.append(df)\n",
    "\n",
    "    # Calculate ROC and AUC and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "## add the diagonal line and set the title and labels\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts_voting[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
