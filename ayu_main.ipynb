{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to use `LabelEncoder` and `OneHotEncoder` to transform categorical data, and `MinMaxScaler` and `StandardScaler` to transform numerical data. After several trials, we decided to use `OneHotEncoder` for categorical data and `MinMaxScaler` for numerical data. The reasons are as follows:\n",
    "\n",
    "1. Categorical data does not have an inherent order. Using `LabelEncoder` might inadvertently create ordinal features.\n",
    "2. Numerical data exhibits significant variance across columns. Using `StandardScaler` might reduce the 'distance' features.\n",
    "\n",
    "Therefore, we opted for `OneHotEncoder` and `MinMaxScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    \n",
    "    # read the data\n",
    "    x_train = pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0)\n",
    "    y_train = pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0)\n",
    "    x_test = pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0)  \n",
    "    \n",
    "    # Initialize Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    OneHotEncoder_encoder = OneHotEncoder()\n",
    "    standard_encoder = StandardScaler()\n",
    "    minmax_encoder = MinMaxScaler()\n",
    "\n",
    "    # seperate the categorical columns and numerical columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for i in x_train.columns:\n",
    "        if x_train[i].dtype == 'float64':\n",
    "            numerical_columns.append(i) #'float64' is the data type of numerical columns\n",
    "        else:\n",
    "            categorical_columns.append(i) #the other type is the data type of categorical columns\n",
    "            \n",
    "    # copy to avoid changing the original data\n",
    "    X_train_encoded = x_train.copy()\n",
    "    X_test_encoded = x_test.copy()\n",
    "\n",
    "    ## == apply one hot encoding to categorical columns == ##\n",
    "    ## =================================================== ##\n",
    "    for col in categorical_columns:\n",
    "        # Fit the one hot encoder on the combined data of train and test to avoid unseen labels\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        OneHotEncoder_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        # Transform the train and test data\n",
    "        ## reshape(-1, 1) is used to convert the 1D array to 2D array\n",
    "        ## The one hot encoder returns a sparse matrix, so we convert it to a dense matrix\n",
    "        train_encoded = OneHotEncoder_encoder.transform(x_train[col].values.reshape(-1, 1)).toarray()\n",
    "        test_encoded = OneHotEncoder_encoder.transform(x_test[col].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "        # Create new column names for the one hot encoded columns\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=[f\"{col}_{int(i)}\" for i in range(train_encoded.shape[1])])\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=[f\"{col}_{int(i)}\" for i in range(test_encoded.shape[1])])\n",
    "\n",
    "        # Concatenate the new one hot encoded columns to the original dataframe\n",
    "        X_train_encoded = pd.concat([X_train_encoded, train_encoded_df], axis=1)\n",
    "        X_test_encoded = pd.concat([X_test_encoded, test_encoded_df], axis=1)\n",
    "\n",
    "        # Drop the original categorical columns after encoding\n",
    "        X_train_encoded.drop(columns=[col], inplace=True)\n",
    "        X_test_encoded.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # for col in categorical_columns:\n",
    "    #     # Fit the label encoder on the combined data of train and test to avoid unseen labels\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     label_encoder.fit(combined_data)\n",
    "\n",
    "    #     X_train_encoded[col] = label_encoder.transform(x_train[col])\n",
    "    #     X_test_encoded[col] = label_encoder.transform(x_test[col])\n",
    "\n",
    "    # #  == apply standard scaler to numerical columns == ##\n",
    "    # for col in numerical_columns:\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     standard_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "    #     X_train_encoded[col] = standard_encoder.fit_transform(x_train[col].values.reshape(-1, 1))\n",
    "    #     X_test_encoded[col] = standard_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    ##  == apply minmax scaler to numerical columns cuz we use XGBOOST and RF== ##\n",
    "    for col in numerical_columns:\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        minmax_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        X_train_encoded[col] = minmax_encoder.transform(x_train[col].values.reshape(-1, 1))\n",
    "        X_test_encoded[col] = minmax_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(X_train_encoded)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test_encoded)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the encoded rightly input in the X_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import  VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. shit way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_names)):\n",
    "\n",
    "    ## First: use Filter method to remove features with low variance\n",
    "    filter = VarianceThreshold()\n",
    "    X_train_encoded = X_trains[i]\n",
    "    X_test_encoded = X_tests[i]\n",
    "    y_train = y_trains[i]   \n",
    "\n",
    "    # Fit the filter on the train data\n",
    "    filter.fit(X_train_encoded)\n",
    "\n",
    "    # Transform the train and test data\n",
    "    X_train_encoded = filter.transform(X_train_encoded)\n",
    "    X_test_encoded = filter.transform(X_test_encoded)\n",
    "\n",
    "    ## Second: use embedded method to select features\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_encoded, y_train.values.ravel())\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Select features based on importance\n",
    "    selected_features = importances > np.mean(importances)\n",
    "\n",
    "    # Convert the numpy arrays back to DataFrames with the selected feature names\n",
    "    selected_feature_names = X_trains[i].columns[filter.get_support()][selected_features]\n",
    "    X_trains[i] = pd.DataFrame(X_train_encoded[:, selected_features], columns=selected_feature_names)\n",
    "    X_tests[i] = pd.DataFrame(X_test_encoded[:, selected_features], columns=selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. good way but too lo0ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_names)):\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Initialize RFECV\n",
    "    rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "\n",
    "    x_train = X_trains[i]\n",
    "    y_train = y_trains[i]\n",
    "\n",
    "    # Fit RFECV\n",
    "    rfecv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features = x_train.columns[rfecv.support_]\n",
    "\n",
    "    # print(\"Selected features:\", selected_features)\n",
    "\n",
    "    # Transform the datasets\n",
    "    X_trains[i] = X_trains[i][selected_features]\n",
    "    X_tests[i] = X_tests[i][selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before PCA, X_train shape: {X_trains[0].shape}\")\n",
    "print(f\"Before PCA, y_train shape: {y_trains[0].shape}\")\n",
    "\n",
    "# Step 1: 切分資料\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[0], y_trains[0], test_size=0.2, random_state=42)\n",
    "\n",
    "# 確保切分後的樣本數一致\n",
    "print(f\"After train_test_split, X_train_part shape: {X_train_part.shape}\")\n",
    "print(f\"After train_test_split, y_train_part shape: {y_train_part.shape}\")\n",
    "\n",
    "# Step 2: PCA 降維\n",
    "pca = PCA(n_components=0.95)  # 保留 95% 的數據變異\n",
    "X_train_pca = pca.fit_transform(X_train_part)\n",
    "X_valid_pca = pca.transform(X_valid)  # 注意這裡是 transform，不是 fit_transform\n",
    "\n",
    "# 確保降維後樣本數保持一致\n",
    "print(f\"After PCA, X_train_pca shape: {X_train_pca.shape}\")\n",
    "print(f\"After PCA, X_valid_pca shape: {X_valid_pca.shape}\")\n",
    "\n",
    "# Step 3: Mutual Information 特徵選擇\n",
    "mi_scores = mutual_info_classif(X_train_pca, y_train_part.values.ravel())  # 計算互信息分數\n",
    "mi_threshold = np.mean(mi_scores)  # 使用平均值作為閾值\n",
    "selected_mi_features = mi_scores > mi_threshold\n",
    "\n",
    "# 保留選中的特徵\n",
    "X_train_mi = X_train_pca[:, selected_mi_features]\n",
    "X_valid_mi = X_valid_pca[:, selected_mi_features]  # 記得對驗證集也做相同的選擇\n",
    "\n",
    "# 確保選擇後特徵數量正確\n",
    "print(f\"互信息篩選後的特徵數量: {X_train_mi.shape[1]}\")\n",
    "\n",
    "# Step 4: LightGBM 嵌入式方法\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_mi, y_train_part.values.ravel())  # 訓練模型\n",
    "\n",
    "# 使用特徵重要性進行篩選\n",
    "feature_importances = lgb_model.feature_importances_\n",
    "importance_threshold = np.mean(feature_importances)  # 使用平均值作為閾值\n",
    "selected_lgb_features = feature_importances > importance_threshold\n",
    "\n",
    "X_train_final = X_train_mi[:, selected_lgb_features]\n",
    "X_valid_final = X_valid_mi[:, selected_lgb_features]  # 記得對驗證集也進行相同操作\n",
    "\n",
    "# 確保最終特徵數量\n",
    "print(f\"LightGBM 篩選後的特徵數量: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Step 5: 將最終特徵轉回 DataFrame\n",
    "X_train_final_df = pd.DataFrame(X_train_final, columns=[f\"Feature_{i+1}\" for i in range(X_train_final.shape[1])])\n",
    "X_valid_final_df = pd.DataFrame(X_valid_final, columns=[f\"Feature_{i+1}\" for i in range(X_valid_final.shape[1])])\n",
    "\n",
    "# 更新 X_trains 和 X_tests\n",
    "X_trains[0] = pd.concat([X_train_final_df,X_valid_final_df],axis=0) \n",
    "\n",
    "# 顯示最終的特徵\n",
    "print(f\"Final features in X_train: {X_trains[0].shape}\")\n",
    "print(f\"Final features in X_test: {X_tests[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_predicts_voting = []\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[i], y_trains[i], test_size=0.2, random_state=42)\n",
    "    \n",
    "    ##  Boosting first, Bagging second, Stacking third\n",
    "\n",
    "    # 1.Boosting first: Initialize the boosting models\n",
    "    boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "    boosting_clf3 = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "    boosting_clf4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    boosting_clf5 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    boosting_clf6 = SVC(probability=True, random_state=42)\n",
    "    boosting_clf8 = GaussianNB()\n",
    "    boosting_clf10 = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    # 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "    votin_clf= VotingClassifier(estimators=[\n",
    "                ('gb1', boosting_clf1),\n",
    "                ('gb2', boosting_clf2),\n",
    "                ('gb3', boosting_clf3),\n",
    "                ('gb4', boosting_clf4),\n",
    "                ('gb5', boosting_clf5),\n",
    "                ('gb6', boosting_clf6),\n",
    "                ('gb8', boosting_clf8),\n",
    "                ('gb10', boosting_clf10)],voting='soft', n_jobs=-1)\n",
    "\n",
    "\n",
    "    bagging_model = BaggingClassifier(estimator=votin_clf,n_estimators=10,random_state=42,n_jobs=-1)\n",
    "\n",
    "    # 3.Stacking third: Initialize the stacking model\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('bagging', bagging_model)  # Bagging 模型作為 Stacking 的基分類器之一\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(),  # Meta-model\n",
    "        passthrough=True  # 保留原始特徵與基分類器的輸出\n",
    "    )\n",
    "\n",
    "    # train the stacking model\n",
    "    stacking_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "\n",
    "    # 使用驗證集進行預測\n",
    "    y_pred = stacking_model.predict(X_valid)\n",
    "    y_pred_proba = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # 評估模型\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "    print(f\"{i} times finish\")\n",
    "    print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "    # 使用訓練好的Voting模型來預測X_test並儲存結果\n",
    "    y_test_pred = stacking_model.predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "    y_predicts_voting.append(df)\n",
    "\n",
    "    # 繪製AUC曲線\n",
    "    # Calculate ROC and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts_voting[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
