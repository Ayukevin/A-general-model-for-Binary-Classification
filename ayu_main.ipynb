{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "\n",
    "for folder_name in os.listdir(\"./Competition_data\"):\n",
    "    \n",
    "    x_train = pd.read_csv(f\"./Competition_data/{folder_name}/X_train.csv\",header=0)\n",
    "    y_train = pd.read_csv(f\"./Competition_data/{folder_name}/y_train.csv\",header=0)\n",
    "    x_test = pd.read_csv(f\"./Competition_data/{folder_name}/X_test.csv\",header=0)  \n",
    "    \n",
    "    # Initialize Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    OneHotEncoder_encoder = OneHotEncoder()\n",
    "    standard_encoder = StandardScaler()\n",
    "    minmax_encoder = MinMaxScaler()\n",
    "\n",
    "    # seperate the categorical columns and numerical columns\n",
    "    numerical_columns = []\n",
    "    categorical_columns = []\n",
    "    for i in x_train.columns:\n",
    "        if x_train[i].dtype == 'float64':\n",
    "            numerical_columns.append(i)\n",
    "        else:\n",
    "            categorical_columns.append(i)\n",
    "            \n",
    "    # copy to avoid changing the original data\n",
    "    X_train_encoded = x_train.copy()\n",
    "    X_test_encoded = x_test.copy()\n",
    "\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        # Fit the one hot encoder on the combined data of train and test to avoid unseen labels\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        OneHotEncoder_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        # Transform the train and test data\n",
    "        train_encoded = OneHotEncoder_encoder.transform(x_train[col].values.reshape(-1, 1)).toarray()\n",
    "        test_encoded = OneHotEncoder_encoder.transform(x_test[col].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "        # Create new column names for the one hot encoded columns\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=[f\"{col}_{int(i)}\" for i in range(train_encoded.shape[1])])\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=[f\"{col}_{int(i)}\" for i in range(test_encoded.shape[1])])\n",
    "\n",
    "        # Concatenate the new one hot encoded columns to the original dataframe\n",
    "        X_train_encoded = pd.concat([X_train_encoded, train_encoded_df], axis=1)\n",
    "        X_test_encoded = pd.concat([X_test_encoded, test_encoded_df], axis=1)\n",
    "\n",
    "        # Drop the original categorical columns\n",
    "        X_train_encoded.drop(columns=[col], inplace=True)\n",
    "        X_test_encoded.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # for col in categorical_columns:\n",
    "    #     # Fit the label encoder on the combined data of train and test to avoid unseen labels\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     label_encoder.fit(combined_data)\n",
    "\n",
    "    #     X_train_encoded[col] = label_encoder.transform(x_train[col])\n",
    "    #     X_test_encoded[col] = label_encoder.transform(x_test[col])\n",
    "\n",
    "    # #  == apply standard scaler to numerical columns == ##\n",
    "    # for col in numerical_columns:\n",
    "    #     combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "    #     standard_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "    #     X_train_encoded[col] = standard_encoder.fit_transform(x_train[col].values.reshape(-1, 1))\n",
    "    #     X_test_encoded[col] = standard_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    ##  == apply minmax scaler to numerical columns cuz we use XGBOOST and RF== ##\n",
    "    for col in numerical_columns:\n",
    "        combined_data = pd.concat([x_train[col], x_test[col]], axis=0)\n",
    "        minmax_encoder.fit(combined_data.values.reshape(-1, 1))\n",
    "\n",
    "        X_train_encoded[col] = minmax_encoder.transform(x_train[col].values.reshape(-1, 1))\n",
    "        X_test_encoded[col] = minmax_encoder.transform(x_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(X_train_encoded)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test_encoded)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import  VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_names)):\n",
    "\n",
    "    ## First: use Filter method to remove features with low variance\n",
    "    filter = VarianceThreshold()\n",
    "    X_train_encoded = X_trains[i]\n",
    "    X_test_encoded = X_tests[i]\n",
    "    y_train = y_trains[i]   \n",
    "\n",
    "    # Fit the filter on the train data\n",
    "    filter.fit(X_train_encoded)\n",
    "\n",
    "    # Transform the train and test data\n",
    "    X_train_encoded = filter.transform(X_train_encoded)\n",
    "    X_test_encoded = filter.transform(X_test_encoded)\n",
    "\n",
    "    ## Second: use embedded method to select features\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train_encoded, y_train.values.ravel())\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Select features based on importance\n",
    "    selected_features = importances > np.mean(importances)\n",
    "\n",
    "    # Convert the numpy arrays back to DataFrames with the selected feature names\n",
    "    selected_feature_names = X_trains[i].columns[filter.get_support()][selected_features]\n",
    "    X_trains[i] = pd.DataFrame(X_train_encoded[:, selected_features], columns=selected_feature_names)\n",
    "    X_tests[i] = pd.DataFrame(X_test_encoded[:, selected_features], columns=selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_predicts_voting = []\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(X_trains[i], y_trains[i], test_size=0.2, random_state=42)\n",
    "    \n",
    "    ##  Boosting first, Bagging second, Stacking third\n",
    "\n",
    "    # 1.Boosting first: Initialize the boosting models\n",
    "    boosting_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    boosting_clf2 = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "    boosting_clf3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "    # 2.Bagging second: use VotingClassifier to combine the boosting models\n",
    "    votin_clf= VotingClassifier(estimators=[\n",
    "                ('gb1', boosting_clf1),\n",
    "                ('gb2', boosting_clf2),\n",
    "                ('gb3', boosting_clf3)],voting='soft', n_jobs=-1)\n",
    "\n",
    "\n",
    "    bagging_model = BaggingClassifier(estimator=votin_clf,n_estimators=10,random_state=42,n_jobs=-1)\n",
    "\n",
    "    # 3.Stacking third: Initialize the stacking model\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('bagging', bagging_model)  # Bagging 模型作為 Stacking 的基分類器之一\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(),  # Meta-model\n",
    "        passthrough=True  # 保留原始特徵與基分類器的輸出\n",
    "    )\n",
    "\n",
    "    # train the stacking model\n",
    "    stacking_model.fit(X_train_part, y_train_part)\n",
    "\n",
    "\n",
    "    # 使用驗證集進行預測\n",
    "    y_pred = stacking_model.predict(X_valid)\n",
    "    y_pred_proba = stacking_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # 評估模型\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc1 = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "    print(f\"{i} times finish\")\n",
    "    print(f\"AUC: {roc_auc1:.2f}\")\n",
    "\n",
    "    # 使用訓練好的Voting模型來預測X_test並儲存結果\n",
    "    y_test_pred = stacking_model.predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_test_pred, columns=['y_predict_proba'])\n",
    "    y_predicts_voting.append(df)\n",
    "\n",
    "    # 繪製AUC曲線\n",
    "    # Calculate ROC and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{dataset_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df=y_predicts_voting[idx]\n",
    "    df.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
